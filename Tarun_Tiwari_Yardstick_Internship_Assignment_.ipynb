{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### **Assignment:** Conversation Management & Classification using Groq API"
      ],
      "metadata": {
        "id": "e4wIoHUjvVZd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing API Response\n",
        "\n",
        "import os\n",
        "import openai\n",
        "\n",
        "client = openai.OpenAI(\n",
        "    base_url = \"https://api.groq.com/openai/v1\",\n",
        "    api_key = \"my api key\" # Not showing API on Github due to security issues but it is available on Colab Link I provided you.\n",
        ")"
      ],
      "metadata": {
        "id": "H1w3OpwZ5FWO"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    response = client.chat.completions.create(\n",
        "    model=\"openai/gpt-oss-20b\", # selecting a valid Grok Model\n",
        "    messages=[{\"role\": \"user\", \"content\": \"hello\"}]\n",
        "    )\n",
        "    print(response.choices[0].message.content)\n",
        "except Exception as e:\n",
        "    print(\"Error:\", e)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_z1xceru7hqp",
        "outputId": "e2e42447-ec4b-4aa3-d1f3-7d176b42591e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello! ðŸ‘‹ How can I help you today?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Task 1:** Managing Conversation History with Summarization\n",
        "\n",
        "* We defined a Conversation_Manager class to maintain the ongoing chat history between user and assistant.\n",
        "* To keep the context manageable and optimize performance, the history is truncated based on configurable limits (either by the number of conversation turns or total character length)\n",
        "* Additionally, after every k-th turn (configurable), the conversation is summarized using the Groq GPT model (openai/gpt-oss-20b), replacing previous messages with a concise summary.\n",
        "\n",
        "This approach balances maintaining relevant context while preventing uncontrolled growth of conversation data."
      ],
      "metadata": {
        "id": "pzWILS8AvdoB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Conversation_Manager:\n",
        "    \"\"\"\n",
        "    Manages conversation history between user and assistant.\n",
        "    Supports:\n",
        "    - Adding new turns to history.\n",
        "    - Truncation by max turns or max characters.\n",
        "    - Periodic summarization after every k-th run.\n",
        "    \"\"\"\n",
        "    def __init__(self, summarization_frequency=3, max_turns=None, max_chars=None):\n",
        "        self.history = []\n",
        "        self.run_count = 0\n",
        "        self.summarization_frequency = summarization_frequency\n",
        "        self.max_turns = max_turns\n",
        "        self.max_chars = max_chars\n",
        "\n",
        "    def add_message(self, role, content):  # appends a message dict. with role and text content to history\n",
        "        self.history.append({\"role\": role, \"content\": content})\n",
        "\n",
        "    def truncate_history(self):\n",
        "        \"\"\"\n",
        "        - If max_turns specified, keeps only last max_turns messages.\n",
        "        - If max_chars specified, removes oldest messages until total text length <= max_chars.\n",
        "        \"\"\"\n",
        "        if self.max_turns and len(self.history) > self.max_turns:\n",
        "            self.history = self.history[-self.max_turns:]\n",
        "        if self.max_chars:\n",
        "            total_len = sum(len(m['content']) for m in self.history)\n",
        "            while total_len > self.max_chars and self.history:\n",
        "                removed = self.history.pop(0)\n",
        "                total_len -= len(removed['content'])\n",
        "\n",
        "    def summarize_history(self):\n",
        "        \"\"\"\n",
        "        this will combine all messages into a prompt, requests summary and then\n",
        "        replaces history with single summary message.\n",
        "        \"\"\"\n",
        "        prompt = \"Summarize this conversation briefly:\\n\\n\"\n",
        "        for msg in self.history:\n",
        "            prompt += f\"{msg['role'].capitalize()}: {msg['content']}\\n\"\n",
        "\n",
        "        response = client.chat.completions.create(\n",
        "            model=\"openai/gpt-oss-20b\",  # using a model which is compatible with Grok\n",
        "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "            max_tokens=150,\n",
        "            temperature=0.5,\n",
        "        )\n",
        "        summary = response.choices[0].message.content\n",
        "        # Now this will replace entire history with this summary as a single message\n",
        "        self.history = [{\"role\": \"assistant\", \"content\": summary}]\n",
        "        return summary\n",
        "\n",
        "    def process_new_turn(self, user_message, assistant_response):\n",
        "        \"\"\"\n",
        "        - Process a new conversation turn: add messages, truncate, could possibly summarize.\n",
        "        - Also summarizes after every k-th run, controlled by summarization_frequency.\n",
        "        \"\"\"\n",
        "        self.run_count += 1\n",
        "        self.add_message(\"user\", user_message)\n",
        "        self.add_message(\"assistant\", assistant_response)\n",
        "        self.truncate_history()\n",
        "\n",
        "        if self.summarization_frequency and (self.run_count % self.summarization_frequency == 0):\n",
        "            summary = self.summarize_history()\n",
        "            return summary\n",
        "        return None\n",
        "\n",
        "    def get_history(self):\n",
        "        return self.history"
      ],
      "metadata": {
        "id": "yw1whn6b78Pw"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Final Demonstartion of Task 1 Functionality->"
      ],
      "metadata": {
        "id": "Az_oVU3KeyRi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "conv_mgr = Conversation_Manager(summarization_frequency=3, max_turns=6, max_chars=1000)\n",
        "\n",
        "samples = [\n",
        "    (\"Hello! How are you?\", \"I am good, thank you! How can I help?\"),\n",
        "    (\"Tell me about AI.\", \"AI is the simulation of human intelligence by machines.\"),\n",
        "    (\"What is OpenAI function calling?\", \"OpenAI function calling lets you define schemas for structured chat output.\"),\n",
        "    (\"Explain JSON schema.\", \"JSON schema defines the structure of JSON data.\"),\n",
        "    (\"Thanks!\", \"You're welcome!\"),\n",
        "]\n",
        "\n",
        "for i, (user_msg, assistant_msg) in enumerate(samples, start=1):\n",
        "    summary = conv_mgr.process_new_turn(user_msg, assistant_msg)\n",
        "    print(f\"Turn {i} added.\")\n",
        "    if summary:  # printing summary on every k-th turn(here 3rd turn)\n",
        "        print(f\"Summary after turn {i}:\\n{summary}\\n\")\n",
        "\n",
        "print(\"Final Conversation History:\")\n",
        "for msg in conv_mgr.get_history():\n",
        "    print(f\"{msg['role'].capitalize()}: {msg['content']}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r5tbcRQTetut",
        "outputId": "196e273d-7d0c-4860-aa44-1d34fa007f34"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Turn 1 added.\n",
            "Turn 2 added.\n",
            "Turn 3 added.\n",
            "Summary after turn 3:\n",
            "The user greets the assistant, then asks two questions: one about what AI is and another about OpenAIâ€™s functionâ€‘calling feature. The assistant replies with brief explanations for each topic.\n",
            "\n",
            "Turn 4 added.\n",
            "Turn 5 added.\n",
            "Final Conversation History:\n",
            "Assistant: The user greets the assistant, then asks two questions: one about what AI is and another about OpenAIâ€™s functionâ€‘calling feature. The assistant replies with brief explanations for each topic.\n",
            "User: Explain JSON schema.\n",
            "Assistant: JSON schema defines the structure of JSON data.\n",
            "User: Thanks!\n",
            "Assistant: You're welcome!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task 1** is completed succesfully as above output fulfils all requirements of **Task 1** mentioned in Assignment"
      ],
      "metadata": {
        "id": "zlFs4fgChNoF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "KdXtB7qaicsI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Task 2:** JSON Schema Classification & Information Extraction\n",
        "\n",
        "* We created a JSON schema specifying key user information fields (name, email, phone, location, age) expected in the chat.\n",
        "* Using Groq APIâ€™s OpenAI-compatible function calling, our code sends user chat samples and requests structured extraction of this information in JSON format following the schema.\n",
        "\n",
        "Each extracted output is validated locally with jsonschema to ensure the data matches the expected types and constraints."
      ],
      "metadata": {
        "id": "uJm_c9qGO-Uy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import jsonschema\n",
        "import json\n",
        "\n",
        "# JSON Schema for defining user info extraction structure\n",
        "\n",
        "json_schema = {\n",
        "    \"type\": \"object\",\n",
        "    \"properties\": {\n",
        "        \"name\": {\"type\": \"string\"},\n",
        "        \"email\": {\"type\": \"string\", \"format\": \"email\"},\n",
        "        \"phone\": {\"type\": \"string\"},\n",
        "        \"location\": {\"type\": \"string\"},\n",
        "        \"age\": {\"type\": \"integer\", \"minimum\": 16, \"maximum\": 120}\n",
        "    },\n",
        "    \"required\": []\n",
        "}\n",
        "\n",
        "tools = [\n",
        "    {\n",
        "        \"type\": \"function\",\n",
        "        \"function\": {\n",
        "            \"name\": \"extract_user_info\",\n",
        "            \"description\": \"Extracts basic user information from chat.\",\n",
        "            \"parameters\": json_schema,\n",
        "        },\n",
        "    }\n",
        "]\n",
        "\n",
        "def call_function_calling_api(chat_text):\n",
        "    \"\"\"\n",
        "    - Calls Groq API with OpenAI-compatible function calling.\n",
        "    - Passes chat_text as input and requests structured extraction\n",
        "      per the JSON schema.\n",
        "    - Returns parsed JSON object from the function call response.\n",
        "    \"\"\"\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"openai/gpt-oss-20b\",\n",
        "        messages=[{\"role\": \"user\", \"content\": chat_text}],\n",
        "        tools=tools,\n",
        "        tool_choice={\"type\": \"function\", \"function\": {\"name\": \"extract_user_info\"}},\n",
        "        temperature=0,\n",
        "    )\n",
        "    resp_message = response.choices[0].message\n",
        "\n",
        "    if hasattr(resp_message, \"tool_calls\") and resp_message.tool_calls:\n",
        "        tool_call = resp_message.tool_calls[0]\n",
        "        try:\n",
        "            func_args = json.loads(tool_call.function.arguments)\n",
        "            return func_args\n",
        "        except json.JSONDecodeError:\n",
        "            print(\"Failed to decode function_call arguments\")\n",
        "            return None\n",
        "    return None\n",
        "\n",
        "def validate_output(data):\n",
        "    \"\"\"\n",
        "    - Validates extracted JSON data against the predefined JSON schema.\n",
        "    - Prints whether the output is valid or displays validation errors.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        jsonschema.validate(instance=data, schema=json_schema)\n",
        "        print(\"Output is valid JSON matching schema.\")\n",
        "    except jsonschema.ValidationError as e:\n",
        "        print(f\"Validation error: {e.message}\")"
      ],
      "metadata": {
        "id": "m3CqurgXOL4N"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# sample chats demonstrating data extraction and validation\n",
        "\n",
        "sample_chats = [\n",
        "    \"Hi, my name is Aman, I am 30 years old and live in Delhi. My email is aman@example.com and phone number 9876543210.\",\n",
        "    \"Hello, Priya here from Mumbai. Reach me at priya@gmail.com.\",\n",
        "    \"Hey, I'm Rahul, aged 28. My contact is rahul28@mail.com.\",\n",
        "]\n",
        "\n",
        "for chat in sample_chats:\n",
        "    print(f\"\\nExtracting from chat: {chat}\")\n",
        "    extracted = call_function_calling_api(chat)\n",
        "    if extracted:\n",
        "        print(\"Extracted JSON:\", extracted)\n",
        "        validate_output(extracted)\n",
        "    else:\n",
        "        print(\"No extraction available.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "853YWFBliL_4",
        "outputId": "0102d2d8-cc59-4d8c-a71d-1cb509f7c6a1"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Extracting from chat: Hi, my name is Aman, I am 30 years old and live in Delhi. My email is aman@example.com and phone number 9876543210.\n",
            "Extracted JSON: {'age': 30, 'email': 'aman@example.com', 'location': 'Delhi', 'name': 'Aman', 'phone': '9876543210'}\n",
            "Output is valid JSON matching schema.\n",
            "\n",
            "Extracting from chat: Hello, Priya here from Mumbai. Reach me at priya@gmail.com.\n",
            "Extracted JSON: {'email': 'priya@gmail.com', 'location': 'Mumbai', 'name': 'Priya'}\n",
            "Output is valid JSON matching schema.\n",
            "\n",
            "Extracting from chat: Hey, I'm Rahul, aged 28. My contact is rahul28@mail.com.\n",
            "Extracted JSON: {'age': 28, 'email': 'rahul28@mail.com'}\n",
            "Output is valid JSON matching schema.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task 2** is also completed sucessfully."
      ],
      "metadata": {
        "id": "ScqEzAD5oOlb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Thank you for reviewing my assignment submission.\n",
        "\n",
        "I have documented the code and logic clearly to reflect my approach and learning journey throughout this project."
      ],
      "metadata": {
        "id": "fVpg9X5-ogsl"
      }
    }
  ]
}